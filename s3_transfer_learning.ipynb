{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Transfer learning for EEG\n",
        "\n",
        "This example shows how to train a neural network with supervision on TUH [2]\n",
        "EEG data and transfer the model to NMT [3] EEG dataset. We follow the approach of [1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: MJ Bayazi <mj.darvishi92@gmail.com>\n",
        "#\n",
        "# License: BSD (3-clause)\n",
        "\n",
        "from braindecode.datautil.serialization import  load_concat_dataset\n",
        "\n",
        "\n",
        "random_state = 2024\n",
        "n_jobs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the dataset\n",
        "\n",
        "### Load  and save the raw recordings\n",
        "\n",
        "Here we assume you already load and preprocess the raw recordings for both TUAB and NMT datasetsand saved the file in `TUAB_path' and 'NMT_path' respectively. To read more see this notebook [here](https://braindecode.org/stable/auto_examples/applied_examples/plot_tuh_eeg_corpus.html) \n",
        "\n",
        "### Load the preprocessed data\n",
        "\n",
        "Now, we load a few recordings from the TUAB dataset. Running\n",
        "this example with more recordings should yield better representations and\n",
        "downstream classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Exp_Path = 'YOUR_PATH'  # specify the path to the experiment folder\n",
        "TUAB_pp_path = Exp_Path + '/tuab/tuab_pp'\n",
        "NMT_pp_path = Exp_Path + '/NMT/nmt_pp'\n",
        "RESULTS_path = Exp_Path + '/results/'\n",
        "N_JOBS = 40  # specify the number of jobs for loading and windowing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load NMT dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mne\n",
        "mne.set_log_level('ERROR')\n",
        "\n",
        "NMT_ds = load_concat_dataset(NMT_pp_path, preload=False,\n",
        "                            target_name=['pathological','age','gender'] ,#)\n",
        "                            ids_to_load=range(200)\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split based on train split from dataset\n",
        "train_set = NMT_ds.split('train')['True']\n",
        "test_set = NMT_ds.split('train')['False']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "print(\"target is being set to pathological clf\")\n",
        "target = NMT_ds.description['pathological'].astype(int)\n",
        "for d, y in zip(NMT_ds.datasets, target):\n",
        "    d.description['pathological'] = y\n",
        "    d.target_name = 'pathological'\n",
        "    d.target = d.description[d.target_name]\n",
        "NMT_ds.set_description(pd.DataFrame([d.description for d in NMT_ds.datasets]), overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the model\n",
        "\n",
        "We can now create the deep learning model. In this tutorial, we use DeepNet introduced in [4]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.models import Deep4Net\n",
        "\n",
        "n_chans = 21\n",
        "n_classes = 2\n",
        "input_window_samples = 6000\n",
        "drop_prob = 0.5\n",
        "cuda = True # Set to False if you don't have a GPU\n",
        "n_start_chans = 25\n",
        "final_conv_length = 1\n",
        "n_chan_factor = 2\n",
        "stride_before_pool = True\n",
        "# input_window_samples =6000\n",
        "model = Deep4Net(\n",
        "            n_chans, n_classes,\n",
        "            n_filters_time=n_start_chans,\n",
        "            n_filters_spat=n_start_chans,\n",
        "            input_window_samples=input_window_samples,\n",
        "            n_filters_2=int(n_start_chans * n_chan_factor),\n",
        "            n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
        "            n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
        "            final_conv_length=final_conv_length,\n",
        "            stride_before_pool=stride_before_pool,\n",
        "            drop_prob=drop_prob)\n",
        "            # Send model to GPU\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "\n",
        "from braindecode.models.util import to_dense_prediction_model, get_output_shape\n",
        "\n",
        "to_dense_prediction_model(model)\n",
        "\n",
        "n_preds_per_input = get_output_shape(model, n_chans, input_window_samples)[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting windows\n",
        "\n",
        "We extract 60-s windows to be used in both datasets. We use a window size of 6000 samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.utils import io\n",
        "from braindecode.datautil.windowers import create_fixed_length_windows\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "        window_train_set = create_fixed_length_windows(train_set, \n",
        "                                                    start_offset_samples=0,\n",
        "                                                    stop_offset_samples=None,\n",
        "                                                    preload=True,\n",
        "                                                    window_size_samples=input_window_samples,\n",
        "                                                    window_stride_samples=n_preds_per_input,\n",
        "                                                    drop_last_window=True,)\n",
        "        \n",
        "with io.capture_output() as captured:\n",
        "        window_test_set = create_fixed_length_windows(test_set,\n",
        "                                                    start_offset_samples=0,\n",
        "                                                    stop_offset_samples=None,preload=False,\n",
        "                                                    window_size_samples=input_window_samples,\n",
        "                                                    window_stride_samples=n_preds_per_input,\n",
        "                                                    drop_last_window=False,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## defining the classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from braindecode import EEGClassifier\n",
        "from braindecode.training.losses import CroppedLoss\n",
        "import torch\n",
        "from skorch.callbacks import LRScheduler\n",
        "from skorch.helper import predefined_split\n",
        "\n",
        "weight_decay = 0.5 * 0.001\n",
        "lr = 0.0625 * 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "clf = EEGClassifier(\n",
        "                model,\n",
        "                cropped=True,\n",
        "                classes=[0, 1],\n",
        "                criterion=CroppedLoss,\n",
        "                criterion__loss_function=torch.nn.functional.nll_loss,\n",
        "                optimizer=torch.optim.AdamW,\n",
        "                train_split=predefined_split(window_test_set), \n",
        "                optimizer__lr=lr,\n",
        "                optimizer__weight_decay=weight_decay,\n",
        "                iterator_train__shuffle=True,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=[\n",
        "                    \"accuracy\", \"balanced_accuracy\",\"f1\",(\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
        "                    ],\n",
        "                device='cuda' if cuda else 'cpu',\n",
        "                )\n",
        "\n",
        "clf.initialize() # This is important!\n",
        "print('classifier initialized')\n",
        "print(\"Number of parameters = \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "We trained our model in S2 on the TUAB. We use similar\n",
        "hyperparameters as in [1]_, but reduce the number of epochs and\n",
        "increase the learning rate to account for the smaller setting of\n",
        "this example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## loading the pre trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_path = RESULTS_path + 'state_dict_2024.pt'\n",
        "state_dicts = torch.load(load_path) \n",
        "model.load_state_dict(state_dicts, strict= False)\n",
        "print('pre-trained model loaded using pytorch')\n",
        "\n",
        "## freeze layers ##\n",
        "freez = False\n",
        "if freez:\n",
        "    for ii, (name, param) in enumerate(model.named_parameters()):\n",
        "        # if 'temporal_block_0' in name or 'temporal_block_1' in name or 'temporal_block_2' in name or 'temporal_block_3' in name: # or 'temporal_block_5' in name or 'conv_classifier' in name:\n",
        "        if not 'conv_classifier' in name:\n",
        "            param.requires_grad = False\n",
        "            print('param:', name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## fine tuning the model on NMT dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from braindecode import EEGClassifier\n",
        "from braindecode.training.losses import CroppedLoss\n",
        "import torch\n",
        "from skorch.callbacks import LRScheduler\n",
        "from skorch.helper import predefined_split\n",
        "\n",
        "weight_decay = 0.5 * 0.001\n",
        "lr = 0.0625 * 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "clf = EEGClassifier(\n",
        "                model,\n",
        "                cropped=True,\n",
        "                classes=[0, 1],\n",
        "                criterion=CroppedLoss,\n",
        "                criterion__loss_function=torch.nn.functional.nll_loss,\n",
        "                optimizer=torch.optim.AdamW,\n",
        "                train_split=predefined_split(window_test_set), \n",
        "                optimizer__lr=lr,\n",
        "                optimizer__weight_decay=weight_decay,\n",
        "                iterator_train__shuffle=True,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=[\n",
        "                    \"accuracy\", \"balanced_accuracy\",\"f1\",(\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
        "                    ],\n",
        "                device='cuda' if cuda else 'cpu',\n",
        "                )\n",
        "\n",
        "clf.initialize() # This is important!\n",
        "print('classifier initialized')\n",
        "print(\"Number of parameters = \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf.fit(window_train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this example, we used transfer learning (TL) as a way to learn\n",
        "representations from a large EEG data and transfer to a smaller dataset. \n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        ".. [1] Darvishi-Bayazi, M. J., Ghaemi, M. S., Lesort, T., Arefin, M. R., Faubert, J., & Rish, I. (2024). Amplifying pathological detection in EEG signaling pathways through cross-dataset transfer learning. Computers in Biology and Medicine, 169, 107893.\n",
        "\n",
        ".. [2] Shawki, N., Shadin, M. G., Elseify, T., Jakielaszek, L., Farkas, T., Persidsky, Y., ... & Picone, J. (2022). Correction to: The temple university hospital digital pathology corpus. In Signal Processing in Medicine and Biology: Emerging Trends in Research and Applications (pp. C1-C1). Cham: Springer International Publishing..\n",
        "\n",
        ".. [3] Khan, H. A., Ul Ain, R., Kamboh, A. M., & Butt, H. T. (2022). The NMT scalp EEG dataset: an open-source annotated dataset of healthy and pathological EEG recordings for predictive modeling. Frontiers in neuroscience, 15, 755817.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
