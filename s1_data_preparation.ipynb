{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Authors: MJ Bayazi <mj.darvishi92@gmail.com>\n",
    "#\n",
    "# License: BSD (3-clause)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Download, Load, Preprocess and Save the TUH and NMT EEG Corpus\n",
    "\n",
    "In this project, we investigate scaling law for transfer learning in normal/abnormal classification on the TUH and NMT EEG Corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import mne\n",
    "mne.set_log_level('ERROR')  # avoid messages everytime a window is extracted\n",
    "\n",
    "from braindecode.preprocessing import (\n",
    "    preprocess, Preprocessor,# create_fixed_length_windows, scale as multiply\n",
    "    )\n",
    "from numpy import multiply\n",
    "\n",
    "from braindecode.datasets.tuh import TUHAbnormal, TUH\n",
    "from nmt import NMT\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp_Path = 'YOUR_PATH'  # specify the path to the experiment folder\n",
    "TUAB_raw_path = Exp_Path + '/tuab/tuab_raw'\n",
    "NMT_raw_path = Exp_Path + '/NMT/nmt_raw'\n",
    "TUAB_pp_path = Exp_Path + '/tuab/tuab_pp'\n",
    "NMT_pp_path = Exp_Path + '/NMT/nmt_pp'\n",
    "RESULTS_path = Exp_Path + '/results/'\n",
    "N_JOBS = 40  # specify the number of jobs for loading and windowing\n",
    "\n",
    "# make all the necessary directories\n",
    "import os\n",
    "os.makedirs(TUAB_raw_path, exist_ok=True)\n",
    "os.makedirs(NMT_raw_path, exist_ok=True)\n",
    "os.makedirs(TUAB_pp_path, exist_ok=True)\n",
    "os.makedirs(NMT_pp_path, exist_ok=True)\n",
    "os.makedirs(RESULTS_path, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download the datasets\n",
    "uncomment the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download datasets\n",
    "# ## TUH\n",
    "# !rsync -auxvL nedc-eeg@www.isip.piconepress.com:data/eeg/tuh_eeg_abnormal/ TUAB_raw_path #you need the password see https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml\n",
    "# ## NMT\n",
    "# url = 'https://drive.google.com/file/d/1jGRm5fzj9pp0pGn_FS7P6kbUhzyiK6O-/view?usp=sharing'\n",
    "# gdown.download(url, NMT_raw_path + '/nmt_scalp_eeg_dataset.zip', quiet=False,fuzzy=True)\n",
    "# !unzip NMT_raw_path + 'nmt_scalp_eeg_dataset.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading TUH and NMT datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'nmt'\n",
    "\n",
    "if dataset == 'tuab':\n",
    "    tuh_ds = TUHAbnormal(\n",
    "    TUAB_raw_path, \n",
    "    target_name=('pathological', 'age', 'gender'),\n",
    "    # recording_ids=range(100),#or None to load the whole dataset,\n",
    "    preload=False,\n",
    "    n_jobs=N_JOBS\n",
    "    )\n",
    "    print(tuh_ds.description)\n",
    "    selected_ds = tuh_ds\n",
    "    PATH_pp = TUAB_pp_path\n",
    "elif dataset == 'tueg':\n",
    "    tueg_ds = TUH(\n",
    "    TUEG_raw_path, \n",
    "    target_name=('gender'),\n",
    "    # recording_ids=range(100),#or None to load the whole dataset,\n",
    "    preload=False,\n",
    "    n_jobs=N_JOBS\n",
    "    )\n",
    "    tueg_ds.description\n",
    "    selected_ds = tueg_ds\n",
    "    PATH_pp = TUEG_pp_path   \n",
    "elif dataset == 'nmt':\n",
    "    nmt_ds = NMT(\n",
    "    NMT_raw_path, \n",
    "    target_name=('pathological', 'age', 'gender'),\n",
    "    # recording_ids=range(100,200),#or None to load the whole dataset,\n",
    "    preload=False,\n",
    "    # n_jobs=N_JOBS\n",
    "    )\n",
    "    nmt_ds.description\n",
    "    selected_ds = nmt_ds\n",
    "    PATH_pp = NMT_pp_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the dataset gives x as ndarray(n_channels x 1) as well as\n",
    "the target as [age of the subject, gender of the subject]. Let's look at the last example\n",
    "as it has more interesting age/gender labels (compare to the last row of the dataframe above).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = selected_ds[-1]\n",
    "print('x:', x.shape)\n",
    "print('y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = selected_ds.datasets[0].raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot_psd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will perform some preprocessing steps. First, we will do some\n",
    "selection of available recordings based on the duration. We will select those\n",
    "recordings, that have at least five minutes duration. Data is not loaded here.\n",
    "Then we will do some basic preprocessings for both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keep_unique (ds):\n",
    "    # keep only unique subjects\n",
    "    df = ds.description\n",
    "\n",
    "    subjects = df['subject']\n",
    "    unique_subjects = df['subject'].unique()\n",
    "    print(f'There are {len(subjects)} subjects in the dataset')\n",
    "    print(f'There are {len(unique_subjects)} unique subjects in the dataset')\n",
    "\n",
    "    # Create a new column 'status' that marks duplicates as 'duplicate' and unique values as 'unique'\n",
    "    df['status'] = df.duplicated('subject')\n",
    "    df['status'] = df['status'].replace({True: 'duplicate', False: 'unique'})\n",
    "\n",
    "    ds.set_description(df, overwrite=True)\n",
    "\n",
    "    # Keep only one record per subject\n",
    "    ds = ds.split('status')['unique']\n",
    "    return ds\n",
    "\n",
    "def remove_common (ds):\n",
    "    # Remove noisy labbled subjects\n",
    "    df = ds.description\n",
    "    # Split the dataframe by pathological column\n",
    "    groups = df.groupby('gender')\n",
    "\n",
    "    # Access each group as a dataframe\n",
    "    df_yes = groups.get_group('F')\n",
    "    df_no = groups.get_group('M')\n",
    "\n",
    "    # Display the first five rows of each group\n",
    "    df_yes.head()\n",
    "    df_no.head()\n",
    "\n",
    "    # Get the unique subjects in each group\n",
    "    subjects_yes = set(df_yes['subject'].unique())\n",
    "    subjects_no = set(df_no['subject'].unique())\n",
    "\n",
    "    # Find the intersection of the two sets\n",
    "    common_subjects = subjects_yes.intersection(subjects_no)\n",
    "\n",
    "    # Display the common subjects\n",
    "    print(f'there are {len(common_subjects)} common subjects in the two pathological groups')\n",
    "\n",
    "    # Filter the dataframe by the common subjects\n",
    "    df_common = df[df['subject'].isin(common_subjects)]\n",
    "\n",
    "    # Display the number of rows of the filtered dataframe\n",
    "    print(f'There are {len(df_common)} out of {len(df)} rows have their status changed')\n",
    "\n",
    "    # Create a new column named 'common' with boolean values\n",
    "    df = df.assign(common=df['subject'].isin(common_subjects))\n",
    "\n",
    "    # Display the first five rows of the dataframe\n",
    "    # df.head()\n",
    "    ds.set_description(df, overwrite=True)\n",
    "\n",
    "    ds = ds.split('common')['False']\n",
    "    \n",
    "    return ds\n",
    "## end of remove noisy labbled subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_duration(ds, tmin=0, tmax=None):\n",
    "    if tmax is None:\n",
    "        tmax = np.inf\n",
    "    # determine length of the recordings and select based on tmin and tmax\n",
    "    split_ids = []\n",
    "    for d_i, d in enumerate(ds.datasets):\n",
    "        duration = d.raw.n_times / d.raw.info['sfreq']\n",
    "        if tmin <= duration <= tmax:\n",
    "            split_ids.append(d_i)\n",
    "    splits = ds.split(split_ids)\n",
    "    split = splits['0']\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = 5 * 60\n",
    "tmax = None\n",
    "\n",
    "remove_common_flag = False\n",
    "\n",
    "if remove_common_flag:\n",
    "    print(len(selected_ds.description))\n",
    "    selected_ds = select_by_duration(selected_ds, tmin, tmax)\n",
    "    print(len(selected_ds.description))\n",
    "    selected_ds = keep_unique(selected_ds)\n",
    "    print(len(selected_ds.description))\n",
    "    selected_ds = remove_common(selected_ds)\n",
    "    print(len(selected_ds.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUH spesific channel PP\n",
    "short_ch_names = sorted([\n",
    "    'A1', 'A2',\n",
    "    'FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
    "    'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'FZ', 'CZ', 'PZ']) \n",
    "ar_ch_names = sorted([\n",
    "    'EEG A1-REF', 'EEG A2-REF',\n",
    "    'EEG FP1-REF', 'EEG FP2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF',\n",
    "    'EEG C4-REF', 'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF',\n",
    "    'EEG F7-REF', 'EEG F8-REF', 'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF',\n",
    "    'EEG T6-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF'])\n",
    "le_ch_names = sorted([\n",
    "    'EEG A1-LE', 'EEG A2-LE',\n",
    "    'EEG FP1-LE', 'EEG FP2-LE', 'EEG F3-LE', 'EEG F4-LE', 'EEG C3-LE',\n",
    "    'EEG C4-LE', 'EEG P3-LE', 'EEG P4-LE', 'EEG O1-LE', 'EEG O2-LE',\n",
    "    'EEG F7-LE', 'EEG F8-LE', 'EEG T3-LE', 'EEG T4-LE', 'EEG T5-LE',\n",
    "    'EEG T6-LE', 'EEG FZ-LE', 'EEG CZ-LE', 'EEG PZ-LE'])\n",
    "assert len(short_ch_names) == len(ar_ch_names) == len(le_ch_names)\n",
    "ar_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n",
    "    ar_ch_names, short_ch_names)}\n",
    "le_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n",
    "    le_ch_names, short_ch_names)}\n",
    "ch_mapping = {'ar': ar_ch_mapping, 'le': le_ch_mapping}\n",
    "\n",
    "\n",
    "def select_by_channels(ds, ch_mapping):\n",
    "    split_ids = []\n",
    "    for i, d in enumerate(ds.datasets):\n",
    "        ref = 'ar' if d.raw.ch_names[0].endswith('-REF') else 'le'\n",
    "        # these are the channels we are looking for\n",
    "        seta = set(ch_mapping[ref].keys())\n",
    "        # these are the channels of the recoding\n",
    "        setb = set(d.raw.ch_names)\n",
    "        # if recording contains all channels we are looking for, include it\n",
    "        if seta.issubset(setb):\n",
    "            split_ids.append(i)\n",
    "    return ds.split(split_ids)['0']\n",
    "\n",
    "if 'tu' in dataset:\n",
    "    selected_ds = select_by_channels(selected_ds, ch_mapping)\n",
    "\n",
    "selected_ds.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asrpy\n",
    "\n",
    "def custom_crop(raw, tmin=0.0, tmax=None, include_tmax=True):\n",
    "    # crop recordings to tmin â€“ tmax. can be incomplete if recording\n",
    "    # has lower duration than tmax\n",
    "    # by default mne fails if tmax is bigger than duration\n",
    "    tmax = min((raw.n_times - 1) / raw.info['sfreq'], tmax)\n",
    "    raw.crop(tmin=tmin, tmax=tmax, include_tmax=include_tmax)\n",
    "\n",
    "def custom_rename_channels(raw, mapping):\n",
    "    # rename channels which are dependent on referencing:\n",
    "    # le: EEG 01-LE, ar: EEG 01-REF\n",
    "    # mne fails if the mapping contains channels as keys that are not present\n",
    "    # in the raw\n",
    "    if 'EEG' in raw.ch_names[0]: #just for tuh\n",
    "        reference = raw.ch_names[0].split('-')[-1].lower()\n",
    "        assert reference in ['le', 'ref'], 'unexpected referencing'\n",
    "        reference = 'le' if reference == 'le' else 'ar'\n",
    "        raw.rename_channels(mapping[reference])\n",
    "def custom_reset_date(raw):\n",
    "    # resolve this error: info[\"meas_date\"] seconds must be between \"(-2147483648, 0)\" and \"(2147483647, 0)\"\n",
    "    print(raw.info[\"meas_date\"])\n",
    "    raw.anonymize()\n",
    "\n",
    "def apply_asr(raw):\n",
    "    try:\n",
    "        # filter the data between 1 and 75 Hz\n",
    "        raw.load_data()\n",
    "        raw.filter(l_freq=1., h_freq=None, fir_design='firwin',\n",
    "            skip_by_annotation='edge')\n",
    "        #run asr \n",
    "        asr = asrpy.ASR(sfreq=raw.info[\"sfreq\"], cutoff=5)\n",
    "        asr.fit(raw.copy())\n",
    "        raw = asr.transform(raw.copy())\n",
    "    except:\n",
    "        print('Could not apply the ASR')\n",
    " \n",
    "# one recording assumed to be equal to one subject (TUAB has multiple sessions per subject)\n",
    "def normalize_one_recording_channel_wise(clean_eeg_data):\n",
    "    \n",
    "    # raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=True)\n",
    "    ch_names = short_ch_names #raw.ch_names\n",
    "\n",
    "    # do your data cleaning/preprocessing pipeline here\n",
    "    # clean_epochs or clean_raw = ...\n",
    "\n",
    "    # for sake of illustration\n",
    "    # clean_eeg_data = raw.get_data()\n",
    "    print(clean_eeg_data.shape)\n",
    "\n",
    "    # compute stats only on clean segments\n",
    "    means = []\n",
    "    stds = []\n",
    "    for i in range(len(ch_names)):\n",
    "        means.append(np.mean(clean_eeg_data[i, :]))\n",
    "        stds.append(np.std(clean_eeg_data[i, :]))\n",
    "    \n",
    "    ## apply clip to 2 stds\n",
    "    #and\n",
    "    # apply z-score normalization to clean_data\n",
    "    normalized_clean_eeg_data = []\n",
    "    for i in range(len(ch_names)):\n",
    "        #   clip to 2 stds\n",
    "        clean_eeg_data[i, :] = np.clip(clean_eeg_data[i, :],a_min=means[i]-2*stds[i],a_max=means[i]+2*stds[i])\n",
    "        #   z-score normalization\n",
    "        # clean_eeg_data[i, :] = (clean_eeg_data[i, :] - means[i]) / stds[i] # zscoring\n",
    "        clean_eeg_data[i, :] = (clean_eeg_data[i, :] - np.mean(clean_eeg_data[i, :])) / np.std(clean_eeg_data[i, :]) # zscoring\n",
    "\n",
    "        ## scaling to be in [0 1] range\n",
    "        # clean_eeg_data[i, :] = (clean_eeg_data[i, :] - min(clean_eeg_data[i, :])) / (max(clean_eeg_data[i, :]) - min(clean_eeg_data[i, :]))\n",
    "    #     normalized_clean_eeg_data.append(normalized_channel)\n",
    "    # normalized_clean_eeg_data = np.array(normalized_clean_eeg_data); print(normalized_clean_eeg_data.shape)\n",
    "\n",
    "    return clean_eeg_data #normalized_clean_eeg_data\n",
    "\n",
    "tmin = 1 * 60\n",
    "tmax = 6 * 60\n",
    "sfreq = 100\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor(custom_crop, tmin=tmin, tmax=tmax, include_tmax=False,\n",
    "                 apply_on_array=False),\n",
    "            \n",
    "    # Preprocessor('set_eeg_reference', ref_channels='average', ch_type='eeg'),\n",
    "    Preprocessor(custom_rename_channels, mapping=ch_mapping,\n",
    "                 apply_on_array=False),\n",
    "    Preprocessor('pick_channels', ch_names=short_ch_names, ordered=True),\n",
    "    # Preprocessor(multiply, factor=1*1e6 if dataset=='nmt' else 1e6, apply_on_array=True),\n",
    "    Preprocessor(lambda data: multiply(data, 1*1e6), apply_on_array=True),  # Convert from V to uV\n",
    "    Preprocessor(custom_reset_date,apply_on_array=False),\n",
    "    Preprocessor(np.clip, a_min=-800, a_max=800, apply_on_array=True),\n",
    "    Preprocessor('resample', sfreq=sfreq),\n",
    "    # Preprocessor(apply_asr,apply_on_array=False),\n",
    "    Preprocessor('set_eeg_reference', ref_channels='average', ch_type='eeg'),\n",
    "    # Preprocessor(normalize_one_recording_channel_wise, apply_on_array=True),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating ds_pp folder\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def create_pp_folder(dir_name):\n",
    "    # Check if directory already exists\n",
    "    if os.path.exists(dir_name):\n",
    "        # Remove directory if it already exists\n",
    "        shutil.rmtree(dir_name)\n",
    "\n",
    "    # Create new directory\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "create_pp_folder(PATH_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_preproc = preprocess(\n",
    "    concat_ds=selected_ds,\n",
    "    preprocessors=preprocessors,\n",
    "    n_jobs=N_JOBS,\n",
    "    save_dir=PATH_pp,\n",
    "    overwrite=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our datasets are preprocessed and saved to the given directories. Now we cann move to the training notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visuliziation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ds.datasets[29].raw.info['ch_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'dataset: {dataset} preprocessing was successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = selected_preproc.datasets[0].raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_preproc.datasets[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw.get_data().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw.get_data().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot(scalings='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot_psd(fmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = selected_preproc.datasets[0].raw\n",
    "print(raw2.get_data().max())\n",
    "print(raw2.get_data().min())\n",
    "raw2.plot(scalings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw3 = selected_preproc.datasets[0].raw\n",
    "print(raw3.get_data().max())\n",
    "print(raw3.get_data().min())\n",
    "raw3.plot(scalings=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
